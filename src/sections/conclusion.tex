%! Author = mkeim
%! Date = 8/13/24

\section{Conclusion}\label{sec:}
In this report, we explored the mechanisms by which words change their meanings over time and how this process can be leveraged to identify \emph{data voids}.
We examined two key studies to gain insights into this phenomenon.

Kulkarni et al.\ (2015) introduced the concept of creating \emph{word embeddings} to measure the context of a word over time.
By constructing these embeddings and applying \emph{change point detection}, they provided a powerful method for identifying significant shifts in word meanings.
This approach can be instrumental in detecting data voids, as abrupt changes in word usage may signal emerging topics or areas where existing data is insufficient or outdated.

Hamilton et al.\ (2016) built on this foundation by applying variations of the word2vec model, particularly using the Skip-Gram with Negative Sampling (SGNS) method,
to track the evolution of word meanings across different time periods.
Hamiltonâ€™s unique contribution lies in the formulation of statistical laws governing semantic change, such as the Law of Conformity and the Law of Innovation.
The \emph{Law of Conformity} indicates that more frequent words tend to undergo smaller semantic changes,
suggesting that less frequent words may be more prone to shifts that could lead to data voids.
Meanwhile, the \emph{Law of Innovation} reveals that words with higher polysemy are more likely to experience significant shifts in meaning.
This insight is crucial for identifying data voids, as words with multiple meanings that rapidly evolve may create gaps in data coverage,
highlighting areas where new data collection or analysis is needed to keep pace with linguistic changes.

Monitoring such shifts can help identify areas where existing data may no longer capture the full spectrum of meanings.